= Calculate the Sizing Requirements for RPM and Containerized Installation

Having the right amount of resources for the Ansible Automation Platform helps to make the platform reliable and reduce cost. Sizing requirements depend on multiple factors, including the type of automation we are planning to run. High level points that should be considered during capacity planning are:

- Characterizing the workloads.
- Reviewing the capabilities of different node types.
- Planning the deployment based on the requirements of the workload.

Below are the isolated factors that play a major role in calculating sizing. 

- Number of the Managed hosts controlled by the AAP. 
- Jobs or Tasks running per hour per host.
- Maximum number of concurrent jobs that you want to support.			
- Maximum number of forks set on jobs. Forks determine the number of hosts that a job acts on concurrently. 						
- Maximum API requests per second.						
- Node size that you prefer to deploy (CPU/Memory/Disk).			

*Unified UI*

Unified UI provides the authentication and load-balancing of requests using the GRPC service. Increasing the CPU and memory will help to reduce bottlenecks when resources are under pressure. You may be having issues with Unified UI capacity if you see 502 gateway errors during access attempts.

*Automation Execution/Controller*

There are 4 types of nodes in the RPM-based Ansible Automation platform: 
- Control Plane: Control node and Hybrid node.
- Execution Plane: Execution node and Hop node. 

Control and hybrid nodes play a key role in managing jobs. These nodes are the "managers" that start jobs and handle their results, feeding the data into a database. Every job needs one Control node to manage it.

Key Concept for control capacity:

- Each job needs 1 unit of control capacity.
- A node with 100 capacity units can control up to 100 jobs at once.

Control nodes can be scaled vertically by adding more CPU and memory (i.e., using a larger virtual machine). This increase Control Plane's ability to:

. manage more jobs simultaneously.
. process job events faster (by processing more events at the same time).

Note: Vertically scaling a Control node does not automatically increase the number of workers that handle web requests. To achieve that, instead of making one Control node larger (vertical scaling), add more Control nodes (horizontal scaling). This spreads the workload and web traffic across multiple nodes using a load balancer. It also adds redundancy in case of node failure. Horizontal scaling is often preferred for better reliability and performance.

Recommended practices for vertical scaling are:

. Scale CPU and memory together, ideally in a 1 CPU to 4 GB RAM ratio.
. Add CPU together with memory even if CPU was not a bottleneck. During high memory utilization, the queue fills up with unprocessed events. Higher CPU capacity helps to process these events faster.

*Execution nodes*
 
Execution and Hybrid nodes provide execution capacity. The capacity consumed by a job running against one host is called fork. The amount of forks consumed by a running job is equal to either the number of forks set in the job template settings or equals the number of hosts in the inventory the job is running against, whichever value is lower.

`Vertically scaling` an Execution node by deploying a larger virtual machine with more resources provides more forks for job execution. This increases the number of concurrent jobs that an instance can run. 				

You can configure an instance group that can only be used for running jobs against a certain Inventory. In this scenario, by `horizontally scaling` the Execution node, you can ensure that lower-priority jobs do not block higher-priority jobs. 

Hop nodes use minimal resources, so vertical scaling doesn't increase capacity. Instead, monitor bandwidth especially for Hop nodes that connect large amounts of Execution nodes. Consider a network upgrade if bandwidth is close to the capacity.

Hop nodes on `horizontal scaling` can increase redundancy and guarantee that traffic continues to flow even in the event of a node failure.

In addition to all the above let's keep in mind that the amount of available forks is calculated based on available memory and CPU.

*Memory Relative Capacity*

`mem_capacity` setting is the maximum amount of memory needed by one fork. Default value is 100 MB. Knowing the value allows calculating the amount of forks available on a specific host. 2 GB of memory is reserved for the AAP services.

[source]
----
For example, 16 GB of available node memory means (16384 - 2048)/100 ~ 140 forks 
----

*CPU Relative Capacity*

Ansible workloads are often processor-bound. In these cases sometimes reducing the simultaneous workload allows more tasks to run faster and reduces the average time-to-completion of those jobs.

Just as the `mem_capacity` defines the amount of memory required per fork, the `cpu_capacity` defines the amount of processing resources required per fork. The baseline value for this is 1 CPU = 4 forks.

[source]
----
4 cpu * 4 forks per cpu = 16 forks
----

Determining the amount of forks available for a specific node is a choice on a scale between the calculated CPU-bound capacity and the memory-bound capacity.

`capacity_adjustment`` value determines how close this choice is to the highest number. The value is between 0 and 1. If set to 1, then the highest number is used. For the calculation examples above adjustment value of 1 means 140 forks. The value of 0 means 16 forks. In case of 0.5 the calculation is:


[source]
----
Capacity = [140 forks memory * 0.50] + [16 forks of CPU * 0.50] 	
	  = 70 + 8 = 78 forks. 
----

Everything discussed so far is applicable to both RPM and Containerized deployments. The only difference between the two is that Containerized deployment doesn't have Control nodes as explained in the previous chapter.

When planning capacity it's important to know the expected workload types and additional components involved. Let's consider the examples in the following two sections.

*Event-driven Ansible Controller / Automation Decision*

Event-driven Ansible workloads include the number of rulebook activations and events being received. The following factors should be considered: 		

- Number of simultaneous rulebook activations 					
- Number of events received by Event-Driven Ansible controller 				

By default, Event-driven Ansible controller allows 12 rulebook activations per node and 24 activations in total can run simultaneously. Even if there are more resources available, this limit will not be exceeded unless overwritten by `MAX_RUNNING_ACTIVATIONS` setting in /etc/ansible-automation-platform/eda/settings.yaml file. Run `automation-eda-controller-service restart` after changing the value.

Memory usage is based on the number of events that the Event-driven Ansible controller has to process. Each rulebook activation container has a 200MB memory limit. For example, with 4 CPUs and 16GB of RAM, one rulebook activation container with an assigned `200MB` memory limit can not handle more than 150,000 events per minute. If the number of parallel running rulebook activations is higher, then the maximum number of events each rulebook activation can process is reduced. If there are too many incoming events at a very high rate, the container can run out of memory trying to process the events. This will kill the container, which causes rulebook activation to fail with a status code of 137.

To increase this limit to 400MB, as an example, open /etc/ansible-automation-platform/eda/settings.yaml, set `PODMAN_MEM_LIMIT = 400m` and restart the `automation-eda-controller-service restart` on the EDA server.

*Automation Content or Private Automation Hub (PAH)* 

PAH provides Execution Environments and collections therefore there is not much need for memory or CPU resources by this component. Meeting the minimum and storage requirements should be enough:

- Number of Execution Environments you have. 
- Size of each execution environment. 
- Number of versions of each execution environment. 

In addition, make sure there is enough Database space for the PAH system. Most of the time the default amount should be enough however the above factors need to be taken into consideration, especially if the requirement is to calculate the amount of space more precisely.
